{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d87a20",
   "metadata": {},
   "source": [
    "# Joint Embeddings (Projection Heads + Species Supervision)\n",
    "\n",
    "This notebook learns a shared audio/video embedding space using projection heads and a multi-positive CLIP-style contrastive loss.\n",
    "It keeps baseline analyses (audio-only, video-only, concat when aligned), UMAP visualizations, clustering metrics, and linear probes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62ee34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddde8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d800de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_emb: (219, 1280) meta_audio: (219, 2)\n",
      "video_emb: (265, 768) meta_video: (265, 2)\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "root = Path(\"/Users/wendycao/fish/processed\")\n",
    "audio_dir = root / \"surfperch_embeddings\"\n",
    "video_dir = root / \"dinov2_embeddings\"\n",
    "\n",
    "out_dir = root / \"joint_embeddings_projection\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proj_dim = 256\n",
    "hidden_dim = 512  # set to None for single Linear\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "epochs = 30\n",
    "steps_per_epoch = 200\n",
    "species_per_batch = 8\n",
    "items_per_species = 4\n",
    "batch_size = species_per_batch * items_per_species\n",
    "\n",
    "tau = 0.07\n",
    "seed = 42\n",
    "split_frac = 0.3  # held-out fraction\n",
    "\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load base embeddings + metadata\n",
    "audio_emb = np.load(audio_dir / \"embeddings.npy\")\n",
    "meta_audio = pd.read_csv(audio_dir / \"metadata.csv\")\n",
    "\n",
    "video_emb = np.load(video_dir / \"embeddings.npy\")\n",
    "meta_video = pd.read_csv(video_dir / \"metadata.csv\")\n",
    "\n",
    "print(\"audio_emb:\", audio_emb.shape, \"meta_audio:\", meta_audio.shape)\n",
    "print(\"video_emb:\", video_emb.shape, \"meta_video:\", meta_video.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11ef1773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split indices to: /Users/wendycao/fish/processed/joint_embeddings_projection\n",
      "Dropped audio samples (singletons): 3\n",
      "Dropped video samples (singletons): 3\n"
     ]
    }
   ],
   "source": [
    "# Labels and train/held-out split\n",
    "\n",
    "def ensure_species_column(df: pd.DataFrame, path_col_candidates=(\"clip_path\", \"path\", \"file_path\")) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"species\" in df.columns:\n",
    "        return df\n",
    "    path_col = None\n",
    "    for c in path_col_candidates:\n",
    "        if c in df.columns:\n",
    "            path_col = c\n",
    "            break\n",
    "    if path_col is None:\n",
    "        raise ValueError(\"No species column and no path column found to derive species.\")\n",
    "    df[\"species\"] = df[path_col].apply(lambda p: Path(p).parent.name)\n",
    "    return df\n",
    "\n",
    "meta_audio = ensure_species_column(meta_audio)\n",
    "meta_video = ensure_species_column(meta_video)\n",
    "\n",
    "if len(meta_audio) != len(audio_emb):\n",
    "    raise ValueError(f\"audio metadata length {len(meta_audio)} != audio embeddings {len(audio_emb)}\")\n",
    "if len(meta_video) != len(video_emb):\n",
    "    raise ValueError(f\"video metadata length {len(meta_video)} != video embeddings {len(video_emb)}\")\n",
    "\n",
    "# Train/held-out split per modality, stratified by species\n",
    "\n",
    "def stratified_split_indices(labels, test_size=0.3, seed=42, min_per_class=2):\n",
    "    labels = np.array(labels)\n",
    "    idx = np.arange(len(labels))\n",
    "\n",
    "    # Drop classes with fewer than min_per_class to avoid stratify errors\n",
    "    counts = pd.Series(labels).value_counts()\n",
    "    keep_labels = counts[counts >= min_per_class].index\n",
    "    keep_mask = np.isin(labels, keep_labels)\n",
    "    dropped = idx[~keep_mask]\n",
    "\n",
    "    if keep_mask.sum() == 0:\n",
    "        raise ValueError(\"No classes with at least 2 samples; cannot stratify.\")\n",
    "\n",
    "    idx_keep = idx[keep_mask]\n",
    "    labels_keep = labels[keep_mask]\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    train_rel, test_rel = next(splitter.split(idx_keep, labels_keep))\n",
    "    train_idx = idx_keep[train_rel]\n",
    "    test_idx = idx_keep[test_rel]\n",
    "\n",
    "    return train_idx, test_idx, dropped\n",
    "\n",
    "species_audio = meta_audio[\"species\"].tolist()\n",
    "species_video = meta_video[\"species\"].tolist()\n",
    "\n",
    "audio_train_idx, audio_test_idx, audio_dropped = stratified_split_indices(\n",
    "    species_audio, test_size=split_frac, seed=seed\n",
    ")\n",
    "video_train_idx, video_test_idx, video_dropped = stratified_split_indices(\n",
    "    species_video, test_size=split_frac, seed=seed\n",
    ")\n",
    "\n",
    "np.save(out_dir / \"audio_train_idx.npy\", audio_train_idx)\n",
    "np.save(out_dir / \"audio_test_idx.npy\", audio_test_idx)\n",
    "np.save(out_dir / \"audio_dropped_idx.npy\", audio_dropped)\n",
    "np.save(out_dir / \"video_train_idx.npy\", video_train_idx)\n",
    "np.save(out_dir / \"video_test_idx.npy\", video_test_idx)\n",
    "np.save(out_dir / \"video_dropped_idx.npy\", video_dropped)\n",
    "\n",
    "print(\"Saved split indices to:\", out_dir)\n",
    "print(\"Dropped audio samples (singletons):\", len(audio_dropped))\n",
    "print(\"Dropped video samples (singletons):\", len(video_dropped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d07f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned pairs for concat baseline: 219\n"
     ]
    }
   ],
   "source": [
    "# Optional: aligned pairs for concat baseline\n",
    "\n",
    "def add_key_column(df, path_col=\"clip_path\"):\n",
    "    keys = []\n",
    "    for p in df[path_col].tolist():\n",
    "        stem = Path(p).stem\n",
    "        if stem.endswith(\"_cropped\"):\n",
    "            stem = stem[:-len(\"_cropped\")]\n",
    "        keys.append(stem)\n",
    "    df = df.copy()\n",
    "    df[\"key\"] = keys\n",
    "    return df\n",
    "\n",
    "concat_available = False\n",
    "concat_audio = None\n",
    "concat_video = None\n",
    "concat_species = None\n",
    "\n",
    "if \"clip_path\" in meta_audio.columns and \"clip_path\" in meta_video.columns:\n",
    "    meta_audio_k = add_key_column(meta_audio, \"clip_path\").reset_index().rename(columns={\"index\": \"audio_idx\"})\n",
    "    meta_video_k = add_key_column(meta_video, \"clip_path\").reset_index().rename(columns={\"index\": \"video_idx\"})\n",
    "\n",
    "    merged = meta_audio_k.merge(\n",
    "        meta_video_k,\n",
    "        on=[\"species\", \"key\"],\n",
    "        suffixes=(\"_audio\", \"_video\"),\n",
    "    )\n",
    "\n",
    "    if len(merged) > 0:\n",
    "        audio_indices = merged[\"audio_idx\"].to_numpy()\n",
    "        video_indices = merged[\"video_idx\"].to_numpy()\n",
    "        concat_audio = audio_emb[audio_indices]\n",
    "        concat_video = video_emb[video_indices]\n",
    "        concat_species = merged[\"species\"].tolist()\n",
    "        concat_available = True\n",
    "\n",
    "    print(\"Aligned pairs for concat baseline:\", len(merged))\n",
    "else:\n",
    "    print(\"No clip_path in one or both metadata files; concat baseline skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f447f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def l2norm(x, eps=1e-12):\n",
    "    return x / (np.linalg.norm(x, axis=1, keepdims=True) + eps)\n",
    "\n",
    "\n",
    "def recall_at_k(query_emb, query_species, db_emb, db_species, ks=(1, 5, 10), batch_size=256, exclude_self=False):\n",
    "    max_k = max(ks)\n",
    "    q_species = np.array(query_species)\n",
    "    db_species = np.array(db_species)\n",
    "\n",
    "    db_t = torch.from_numpy(db_emb).float()\n",
    "    correct = {k: 0 for k in ks}\n",
    "    n = len(query_emb)\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        q = torch.from_numpy(query_emb[i:i+batch_size]).float()\n",
    "        sims = q @ db_t.T\n",
    "\n",
    "        if exclude_self:\n",
    "            # assumes query and db are the same set and aligned by index\n",
    "            for row in range(sims.shape[0]):\n",
    "                sims[row, i + row] = -1e9\n",
    "\n",
    "        topk = torch.topk(sims, k=max_k, dim=1).indices.numpy()\n",
    "\n",
    "        for row, idxs in enumerate(topk):\n",
    "            label = q_species[i + row]\n",
    "            retrieved = db_species[idxs]\n",
    "            for k in ks:\n",
    "                if label in retrieved[:k]:\n",
    "                    correct[k] += 1\n",
    "\n",
    "    return {k: correct[k] / n for k in ks}\n",
    "\n",
    "\n",
    "def cluster_metrics(X, y, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    preds = kmeans.fit_predict(X)\n",
    "    ami = adjusted_mutual_info_score(y, preds)\n",
    "    nmi = normalized_mutual_info_score(y, preds)\n",
    "    return ami, nmi\n",
    "\n",
    "\n",
    "def linear_probe(X, y, train_frac=0.4, seed=42, min_per_class=2):\n",
    "    y = np.array(y)\n",
    "    # Drop classes with fewer than min_per_class samples\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    keep = counts[counts >= min_per_class].index\n",
    "    keep_mask = np.isin(y, keep)\n",
    "    dropped = len(y) - keep_mask.sum()\n",
    "    X = X[keep_mask]\n",
    "    y = y[keep_mask]\n",
    "\n",
    "    if dropped > 0:\n",
    "        print(f\"linear_probe: dropped {dropped} samples from classes with <{min_per_class} examples\")\n",
    "\n",
    "    # Guard against inf/nan\n",
    "    X = np.nan_to_num(X, copy=False, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # If still not stratifiable, fall back to non-stratified split\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    if len(counts) < 2 or counts.min() < 2:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, train_size=train_frac, random_state=seed, shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, train_size=train_frac, random_state=seed, stratify=y\n",
    "        )\n",
    "\n",
    "    # Standardize for numerical stability\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred, average=\"macro\")\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c39b2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection heads + multi-positive contrastive loss\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def sample_species_batch(species_list, audio_map, video_map, m, k, rng):\n",
    "    chosen = rng.choice(species_list, size=m, replace=len(species_list) < m)\n",
    "    a_idx = []\n",
    "    v_idx = []\n",
    "    a_lab = []\n",
    "    v_lab = []\n",
    "    for s in chosen:\n",
    "        a_pool = audio_map[s]\n",
    "        v_pool = video_map[s]\n",
    "        a_pick = rng.choice(a_pool, size=k, replace=len(a_pool) < k)\n",
    "        v_pick = rng.choice(v_pool, size=k, replace=len(v_pool) < k)\n",
    "        a_idx.extend(a_pick)\n",
    "        v_idx.extend(v_pick)\n",
    "        a_lab.extend([s] * k)\n",
    "        v_lab.extend([s] * k)\n",
    "    return np.array(a_idx), np.array(v_idx), np.array(a_lab), np.array(v_lab)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            self.net = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return F.normalize(x, dim=1)\n",
    "\n",
    "\n",
    "def multi_positive_clip_loss(z_a, z_v, y_a, y_v, tau=0.07):\n",
    "    logits = (z_a @ z_v.T) / tau\n",
    "    y_a = y_a.view(-1, 1)\n",
    "    y_v = y_v.view(1, -1)\n",
    "    pos_mask = (y_a == y_v)\n",
    "\n",
    "    neg_inf = torch.finfo(logits.dtype).min\n",
    "\n",
    "    # audio -> video\n",
    "    logits_pos = logits.masked_fill(~pos_mask, neg_inf)\n",
    "    log_prob_pos = torch.logsumexp(logits_pos, dim=1) - torch.logsumexp(logits, dim=1)\n",
    "    valid_a = pos_mask.any(dim=1)\n",
    "    loss_a = -log_prob_pos[valid_a].mean() if valid_a.any() else torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    # video -> audio\n",
    "    logits_t = logits.T\n",
    "    pos_mask_t = pos_mask.T\n",
    "    logits_pos_t = logits_t.masked_fill(~pos_mask_t, neg_inf)\n",
    "    log_prob_pos_t = torch.logsumexp(logits_pos_t, dim=1) - torch.logsumexp(logits_t, dim=1)\n",
    "    valid_v = pos_mask_t.any(dim=1)\n",
    "    loss_v = -log_prob_pos_t[valid_v].mean() if valid_v.any() else torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    return 0.5 * (loss_a + loss_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "707d54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Shared species (train): 43\n",
      "Epoch 01/30  loss=1.1455\n",
      "Epoch 02/30  loss=0.1143\n",
      "Epoch 03/30  loss=0.0291\n",
      "Epoch 04/30  loss=0.0052\n",
      "Epoch 05/30  loss=0.0027\n",
      "Epoch 06/30  loss=0.0020\n",
      "Epoch 07/30  loss=0.0014\n",
      "Epoch 08/30  loss=0.0012\n",
      "Epoch 09/30  loss=0.0010\n",
      "Epoch 10/30  loss=0.0008\n",
      "Epoch 11/30  loss=0.0007\n",
      "Epoch 12/30  loss=0.0006\n",
      "Epoch 13/30  loss=0.0006\n",
      "Epoch 14/30  loss=0.0005\n",
      "Epoch 15/30  loss=0.0004\n",
      "Epoch 16/30  loss=0.0004\n",
      "Epoch 17/30  loss=0.0003\n",
      "Epoch 18/30  loss=0.0003\n",
      "Epoch 19/30  loss=0.0003\n",
      "Epoch 20/30  loss=0.0003\n",
      "Epoch 21/30  loss=0.0002\n",
      "Epoch 22/30  loss=0.0002\n",
      "Epoch 23/30  loss=0.0002\n",
      "Epoch 24/30  loss=0.0002\n",
      "Epoch 25/30  loss=0.0002\n",
      "Epoch 26/30  loss=0.0001\n",
      "Epoch 27/30  loss=0.0001\n",
      "Epoch 28/30  loss=0.0001\n",
      "Epoch 29/30  loss=0.0001\n",
      "Epoch 30/30  loss=0.0001\n"
     ]
    }
   ],
   "source": [
    "# Train projection heads (train split only)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Build species -> indices (train only)\n",
    "audio_by_species = defaultdict(list)\n",
    "for i in audio_train_idx:\n",
    "    audio_by_species[species_audio[i]].append(i)\n",
    "\n",
    "video_by_species = defaultdict(list)\n",
    "for i in video_train_idx:\n",
    "    video_by_species[species_video[i]].append(i)\n",
    "\n",
    "species_shared = sorted(set(audio_by_species) & set(video_by_species))\n",
    "print(\"Shared species (train):\", len(species_shared))\n",
    "\n",
    "if len(species_shared) == 0:\n",
    "    raise ValueError(\"No shared species between audio/video in train split.\")\n",
    "\n",
    "# Map species -> int id for labels\n",
    "species_to_id = {s: i for i, s in enumerate(species_shared)}\n",
    "\n",
    "# Heads\n",
    "audio_head = ProjectionHead(audio_emb.shape[1], proj_dim, hidden_dim=hidden_dim).to(device)\n",
    "video_head = ProjectionHead(video_emb.shape[1], proj_dim, hidden_dim=hidden_dim).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(list(audio_head.parameters()) + list(video_head.parameters()),\n",
    "                        lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    audio_head.train()\n",
    "    video_head.train()\n",
    "    total = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    for _ in range(steps_per_epoch):\n",
    "        a_idx, v_idx, a_lab, v_lab = sample_species_batch(\n",
    "            species_shared, audio_by_species, video_by_species,\n",
    "            species_per_batch, items_per_species, rng\n",
    "        )\n",
    "        a = torch.from_numpy(audio_emb[a_idx]).float().to(device)\n",
    "        v = torch.from_numpy(video_emb[v_idx]).float().to(device)\n",
    "        y_a = torch.from_numpy(np.array([species_to_id[s] for s in a_lab])).long().to(device)\n",
    "        y_v = torch.from_numpy(np.array([species_to_id[s] for s in v_lab])).long().to(device)\n",
    "\n",
    "        z_a = audio_head(a)\n",
    "        z_v = video_head(v)\n",
    "        loss = multi_positive_clip_loss(z_a, z_v, y_a, y_v, tau=tau)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "    avg = total / max(1, n_steps)\n",
    "    print(f\"Epoch {epoch:02d}/{epochs}  loss={avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91c665d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_joint: (219, 256) video_joint: (265, 256)\n",
      "Saved joint embeddings and metadata to: /Users/wendycao/fish/processed/joint_embeddings_projection\n"
     ]
    }
   ],
   "source": [
    "# Export learned joint embeddings (L2-normalized)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_all(emb, head, batch_size=1024):\n",
    "    head.eval()\n",
    "    out = []\n",
    "    for i in range(0, len(emb), batch_size):\n",
    "        x = torch.from_numpy(emb[i:i+batch_size]).float().to(device)\n",
    "        z = head(x).cpu().numpy()\n",
    "        out.append(z)\n",
    "    return np.vstack(out)\n",
    "\n",
    "\n",
    "def l2_normalize(X, eps=1e-12):\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + eps)\n",
    "\n",
    "\n",
    "audio_joint = l2_normalize(encode_all(audio_emb, audio_head))\n",
    "video_joint = l2_normalize(encode_all(video_emb, video_head))\n",
    "\n",
    "print(\"audio_joint:\", audio_joint.shape, \"video_joint:\", video_joint.shape)\n",
    "\n",
    "np.save(out_dir / \"audio_joint.npy\", audio_joint)\n",
    "np.save(out_dir / \"video_joint.npy\", video_joint)\n",
    "\n",
    "# Save metadata with species\n",
    "meta_audio_out = meta_audio.copy()\n",
    "meta_video_out = meta_video.copy()\n",
    "meta_audio_out.to_csv(out_dir / \"audio_metadata.csv\", index=False)\n",
    "meta_video_out.to_csv(out_dir / \"video_metadata.csv\", index=False)\n",
    "\n",
    "print(\"Saved joint embeddings and metadata to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eb030",
   "metadata": {},
   "source": [
    "## Cross-modal retrieval evaluation (held-out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8f0dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio -> Video (held-out): R@1=0.138 R@5=0.262 R@10=0.323 P@1=0.138 P@5=0.098 P@10=0.080 median_rank=17.0\n",
      "Audio -> Video chance (avg species freq in DB): 0.032\n",
      "Video -> Audio (held-out): R@1=0.076 R@5=0.304 R@10=0.481 P@1=0.076 P@5=0.094 P@10=0.090 median_rank=14.0\n",
      "Video -> Audio chance (avg species freq in DB): 0.032\n",
      "Saved examples: /Users/wendycao/fish/processed/joint_embeddings_projection/cross_modal_examples_audio_to_video.csv\n",
      "Saved examples: /Users/wendycao/fish/processed/joint_embeddings_projection/cross_modal_examples_video_to_audio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: divide by zero encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: overflow encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: invalid value encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: divide by zero encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: overflow encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: invalid value encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n"
     ]
    }
   ],
   "source": [
    "# Reusable helpers\n",
    "\n",
    "def l2_normalize(X, eps=1e-12):\n",
    "    return X / (np.linalg.norm(X, axis=1, keepdims=True) + eps)\n",
    "\n",
    "\n",
    "def get_path_column(df, candidates=(\"clip_path\", \"path\", \"file_path\")):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(\"No path column found in metadata.\")\n",
    "\n",
    "\n",
    "def cross_modal_metrics(query_emb, query_species, db_emb, db_species, ks=(1, 5, 10), batch_size=256):\n",
    "    Q = l2_normalize(query_emb)\n",
    "    DB = l2_normalize(db_emb)\n",
    "\n",
    "    db_species_arr = np.array(db_species)\n",
    "    query_species_arr = np.array(query_species)\n",
    "    n_db = len(db_species_arr)\n",
    "\n",
    "    recall = {k: [] for k in ks}\n",
    "    precision = {k: [] for k in ks}\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(0, len(Q), batch_size):\n",
    "        sims = Q[i:i+batch_size] @ DB.T\n",
    "        order = np.argsort(-sims, axis=1)\n",
    "\n",
    "        for row in range(order.shape[0]):\n",
    "            qsp = query_species_arr[i + row]\n",
    "\n",
    "            if not (db_species_arr == qsp).any():\n",
    "                ranks.append(n_db + 1)\n",
    "                for k in ks:\n",
    "                    recall[k].append(0.0)\n",
    "                    precision[k].append(0.0)\n",
    "                continue\n",
    "\n",
    "            ranked_idx = order[row]\n",
    "            ranked_species = db_species_arr[ranked_idx]\n",
    "            match_positions = np.where(ranked_species == qsp)[0]\n",
    "            rank = int(match_positions[0] + 1) if match_positions.size > 0 else n_db + 1\n",
    "            ranks.append(rank)\n",
    "\n",
    "            for k in ks:\n",
    "                topk_species = ranked_species[:k]\n",
    "                recall[k].append(1.0 if qsp in topk_species else 0.0)\n",
    "                precision[k].append(float(np.mean(topk_species == qsp)))\n",
    "\n",
    "    metrics = {\n",
    "        \"recall\": {k: float(np.mean(recall[k])) for k in ks},\n",
    "        \"precision\": {k: float(np.mean(precision[k])) for k in ks},\n",
    "        \"median_rank\": float(np.median(ranks)) if len(ranks) else float(\"nan\"),\n",
    "        \"ranks\": np.array(ranks, dtype=int),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def average_chance(query_species, db_species):\n",
    "    db_counts = pd.Series(db_species).value_counts()\n",
    "    n_db = len(db_species)\n",
    "    if n_db == 0:\n",
    "        return 0.0\n",
    "    chances = [(db_counts.get(s, 0) / n_db) for s in query_species]\n",
    "    return float(np.mean(chances)) if len(chances) else 0.0\n",
    "\n",
    "\n",
    "def save_cross_modal_examples(\n",
    "    query_emb,\n",
    "    query_species,\n",
    "    query_paths,\n",
    "    db_emb,\n",
    "    db_species,\n",
    "    db_paths,\n",
    "    out_csv,\n",
    "    seed=42,\n",
    "    n_samples=20,\n",
    "    topk=5,\n",
    "):\n",
    "    Q = l2_normalize(query_emb)\n",
    "    DB = l2_normalize(db_emb)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = min(n_samples, len(Q))\n",
    "    if n_samples == 0:\n",
    "        print(f\"No queries available for examples: {out_csv}\")\n",
    "        return\n",
    "\n",
    "    sample_idx = rng.choice(len(Q), size=n_samples, replace=False)\n",
    "    rows = []\n",
    "\n",
    "    for qi in sample_idx:\n",
    "        sims = Q[qi] @ DB.T\n",
    "        order = np.argsort(-sims)\n",
    "\n",
    "        q_path = query_paths[qi]\n",
    "        q_species = query_species[qi]\n",
    "\n",
    "        top1 = order[0]\n",
    "        top1_path = db_paths[top1]\n",
    "        top1_species = db_species[top1]\n",
    "        top1_score = float(sims[top1])\n",
    "\n",
    "        topk_list = []\n",
    "        for idx in order[:topk]:\n",
    "            topk_list.append(f\"{db_paths[idx]}|{db_species[idx]}|{sims[idx]:.4f}\")\n",
    "        topk_str = \";\".join(topk_list)\n",
    "\n",
    "        ranked_species = np.array(db_species)[order]\n",
    "        match_positions = np.where(ranked_species == q_species)[0]\n",
    "        rank = int(match_positions[0] + 1) if match_positions.size > 0 else len(db_species) + 1\n",
    "\n",
    "        rows.append({\n",
    "            \"query_clip_path\": q_path,\n",
    "            \"query_species\": q_species,\n",
    "            \"top1_clip_path\": top1_path,\n",
    "            \"top1_species\": top1_species,\n",
    "            \"top1_score\": top1_score,\n",
    "            \"top5\": topk_str,\n",
    "            \"rank_of_first_same_species\": rank,\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(\"Saved examples:\", out_csv)\n",
    "\n",
    "\n",
    "# Build held-out query/db splits\n",
    "path_audio = get_path_column(meta_audio)\n",
    "path_video = get_path_column(meta_video)\n",
    "\n",
    "audio_test_idx = np.array(audio_test_idx)\n",
    "video_test_idx = np.array(video_test_idx)\n",
    "\n",
    "audio_q = audio_joint[audio_test_idx]\n",
    "video_q = video_joint[video_test_idx]\n",
    "\n",
    "video_db = video_joint[video_test_idx]\n",
    "audio_db = audio_joint[audio_test_idx]\n",
    "\n",
    "audio_q_species = meta_audio.loc[audio_test_idx, \"species\"].to_numpy()\n",
    "video_q_species = meta_video.loc[video_test_idx, \"species\"].to_numpy()\n",
    "\n",
    "video_db_species = meta_video.loc[video_test_idx, \"species\"].to_numpy()\n",
    "audio_db_species = meta_audio.loc[audio_test_idx, \"species\"].to_numpy()\n",
    "\n",
    "audio_q_paths = meta_audio.loc[audio_test_idx, path_audio].to_numpy()\n",
    "video_q_paths = meta_video.loc[video_test_idx, path_video].to_numpy()\n",
    "\n",
    "video_db_paths = meta_video.loc[video_test_idx, path_video].to_numpy()\n",
    "audio_db_paths = meta_audio.loc[audio_test_idx, path_audio].to_numpy()\n",
    "\n",
    "# Metrics\n",
    "metrics_a2v = cross_modal_metrics(audio_q, audio_q_species, video_db, video_db_species)\n",
    "metrics_v2a = cross_modal_metrics(video_q, video_q_species, audio_db, audio_db_species)\n",
    "\n",
    "chance_a2v = average_chance(audio_q_species, video_db_species)\n",
    "chance_v2a = average_chance(video_q_species, audio_db_species)\n",
    "\n",
    "print(\"Audio -> Video (held-out):\",\n",
    "      f\"R@1={metrics_a2v['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_a2v['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_a2v['recall'][10]:.3f}\",\n",
    "      f\"P@1={metrics_a2v['precision'][1]:.3f}\",\n",
    "      f\"P@5={metrics_a2v['precision'][5]:.3f}\",\n",
    "      f\"P@10={metrics_a2v['precision'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_a2v['median_rank']:.1f}\")\n",
    "print(\"Audio -> Video chance (avg species freq in DB):\", f\"{chance_a2v:.3f}\")\n",
    "\n",
    "print(\"Video -> Audio (held-out):\",\n",
    "      f\"R@1={metrics_v2a['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_v2a['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_v2a['recall'][10]:.3f}\",\n",
    "      f\"P@1={metrics_v2a['precision'][1]:.3f}\",\n",
    "      f\"P@5={metrics_v2a['precision'][5]:.3f}\",\n",
    "      f\"P@10={metrics_v2a['precision'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_v2a['median_rank']:.1f}\")\n",
    "print(\"Video -> Audio chance (avg species freq in DB):\", f\"{chance_v2a:.3f}\")\n",
    "\n",
    "# Qualitative retrieval examples\n",
    "save_cross_modal_examples(\n",
    "    audio_q,\n",
    "    audio_q_species,\n",
    "    audio_q_paths,\n",
    "    video_db,\n",
    "    video_db_species,\n",
    "    video_db_paths,\n",
    "    out_dir / \"cross_modal_examples_audio_to_video.csv\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "save_cross_modal_examples(\n",
    "    video_q,\n",
    "    video_q_species,\n",
    "    video_q_paths,\n",
    "    audio_db,\n",
    "    audio_db_species,\n",
    "    audio_db_paths,\n",
    "    out_dir / \"cross_modal_examples_video_to_audio.csv\",\n",
    "    seed=seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93a4bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding baseline (not a learned joint space)\n",
      "Audio -> Video (padded, full set) R@1=0.009 R@5=0.087 R@10=0.132 median_rank=34.0\n",
      "Video -> Audio (padded, full set) R@1=0.011 R@5=0.091 R@10=0.325 median_rank=19.0\n",
      "Shuffle sanity check (padded video permuted)\n",
      "Audio -> Video (shuffled, full set) R@1=0.046 R@5=0.169 R@10=0.265 median_rank=27.0\n",
      "Video -> Audio (shuffled, full set) R@1=0.030 R@5=0.091 R@10=0.257 median_rank=26.0\n",
      "Saved examples: /Users/wendycao/fish/processed/joint_embeddings_projection/padding_baseline_examples_audio_to_video.csv\n",
      "Saved examples: /Users/wendycao/fish/processed/joint_embeddings_projection/padding_baseline_examples_video_to_audio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: divide by zero encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: overflow encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:27: RuntimeWarning: invalid value encountered in matmul\n",
      "  sims = Q[i:i+batch_size] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: divide by zero encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: overflow encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n",
      "/var/folders/xs/2mdlsqc56b76kyrbbjkjvyvr0000gn/T/ipykernel_6169/2432879045.py:94: RuntimeWarning: invalid value encountered in matmul\n",
      "  sims = Q[qi] @ DB.T\n"
     ]
    }
   ],
   "source": [
    "# Padding baseline (not a learned joint space)\n",
    "\n",
    "# Raw embeddings (audio 1280-D, video 768-D)\n",
    "audio_raw = audio_emb.astype(np.float32)\n",
    "video_raw = video_emb.astype(np.float32)\n",
    "\n",
    "if audio_raw.ndim != 2 or video_raw.ndim != 2:\n",
    "    raise ValueError(f\"Expected 2D embeddings, got audio {audio_raw.shape}, video {video_raw.shape}\")\n",
    "if audio_raw.shape[1] != 1280:\n",
    "    print(f\"Warning: expected audio dim 1280, got {audio_raw.shape[1]}\")\n",
    "if video_raw.shape[1] != 768:\n",
    "    print(f\"Warning: expected video dim 768, got {video_raw.shape[1]}\")\n",
    "\n",
    "def pad_video_to_dim(video_emb, target_dim=1280):\n",
    "    n, d = video_emb.shape\n",
    "    if d > target_dim:\n",
    "        raise ValueError(f\"Video dim {d} exceeds target dim {target_dim}\")\n",
    "    if d == target_dim:\n",
    "        return video_emb\n",
    "    pad = target_dim - d\n",
    "    return np.pad(video_emb, ((0, 0), (0, pad)), mode='constant')\n",
    "\n",
    "video_padded = pad_video_to_dim(video_raw, target_dim=audio_raw.shape[1])\n",
    "\n",
    "# Full-set retrieval (no training, so evaluate on all samples)\n",
    "audio_q_pad = audio_raw\n",
    "video_q_pad = video_padded\n",
    "\n",
    "video_db_pad = video_padded\n",
    "audio_db_pad = audio_raw\n",
    "\n",
    "audio_q_species_full = meta_audio[\"species\"].to_numpy()\n",
    "video_q_species_full = meta_video[\"species\"].to_numpy()\n",
    "\n",
    "video_db_species_full = meta_video[\"species\"].to_numpy()\n",
    "audio_db_species_full = meta_audio[\"species\"].to_numpy()\n",
    "\n",
    "# Metrics\n",
    "metrics_pad_a2v = cross_modal_metrics(audio_q_pad, audio_q_species_full, video_db_pad, video_db_species_full)\n",
    "metrics_pad_v2a = cross_modal_metrics(video_q_pad, video_q_species_full, audio_db_pad, audio_db_species_full)\n",
    "\n",
    "print(\"Padding baseline (not a learned joint space)\")\n",
    "print(\"Audio -> Video (padded, full set)\",\n",
    "      f\"R@1={metrics_pad_a2v['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_pad_a2v['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_pad_a2v['recall'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_pad_a2v['median_rank']:.1f}\")\n",
    "print(\"Video -> Audio (padded, full set)\",\n",
    "      f\"R@1={metrics_pad_v2a['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_pad_v2a['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_pad_v2a['recall'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_pad_v2a['median_rank']:.1f}\")\n",
    "\n",
    "# Shuffle sanity check: permute padded video embeddings\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "perm_db = rng.permutation(len(video_db_pad))\n",
    "video_db_pad_shuf = video_db_pad[perm_db]\n",
    "metrics_pad_a2v_shuf = cross_modal_metrics(audio_q_pad, audio_q_species_full, video_db_pad_shuf, video_db_species_full)\n",
    "\n",
    "perm_q = rng.permutation(len(video_q_pad))\n",
    "video_q_pad_shuf = video_q_pad[perm_q]\n",
    "metrics_pad_v2a_shuf = cross_modal_metrics(video_q_pad_shuf, video_q_species_full, audio_db_pad, audio_db_species_full)\n",
    "\n",
    "print(\"Shuffle sanity check (padded video permuted)\")\n",
    "print(\"Audio -> Video (shuffled, full set)\",\n",
    "      f\"R@1={metrics_pad_a2v_shuf['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_pad_a2v_shuf['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_pad_a2v_shuf['recall'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_pad_a2v_shuf['median_rank']:.1f}\")\n",
    "print(\"Video -> Audio (shuffled, full set)\",\n",
    "      f\"R@1={metrics_pad_v2a_shuf['recall'][1]:.3f}\",\n",
    "      f\"R@5={metrics_pad_v2a_shuf['recall'][5]:.3f}\",\n",
    "      f\"R@10={metrics_pad_v2a_shuf['recall'][10]:.3f}\",\n",
    "      f\"median_rank={metrics_pad_v2a_shuf['median_rank']:.1f}\")\n",
    "\n",
    "# Qualitative retrieval examples (padding baseline)\n",
    "save_cross_modal_examples(\n",
    "    audio_q_pad,\n",
    "    audio_q_species_full,\n",
    "    meta_audio[get_path_column(meta_audio)].to_numpy(),\n",
    "    video_db_pad,\n",
    "    video_db_species_full,\n",
    "    meta_video[get_path_column(meta_video)].to_numpy(),\n",
    "    out_dir / \"padding_baseline_examples_audio_to_video.csv\",\n",
    "    seed=seed,\n",
    "    n_samples=20,\n",
    "    topk=5,\n",
    ")\n",
    "\n",
    "save_cross_modal_examples(\n",
    "    video_q_pad,\n",
    "    video_q_species_full,\n",
    "    meta_video[get_path_column(meta_video)].to_numpy(),\n",
    "    audio_db_pad,\n",
    "    audio_db_species_full,\n",
    "    meta_audio[get_path_column(meta_audio)].to_numpy(),\n",
    "    out_dir / \"padding_baseline_examples_video_to_audio.csv\",\n",
    "    seed=seed,\n",
    "    n_samples=20,\n",
    "    topk=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "548042ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AMI/NMI ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-only | AMI=0.0944  NMI=0.5638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video-only | AMI=0.4630  NMI=0.7196\n",
      "joint(audio) | AMI=0.5754  NMI=0.8135\n",
      "joint(video) | AMI=0.7605  NMI=0.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint(combined) | AMI=0.6963  NMI=0.8076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat | AMI=0.3722  NMI=0.7143\n"
     ]
    }
   ],
   "source": [
    "# Cluster metrics (AMI/NMI)\n",
    "\n",
    "# Ensure base embeddings are available\n",
    "if 'audio_base' not in globals():\n",
    "    audio_base = l2norm(audio_emb.astype(np.float32))\n",
    "if 'video_base' not in globals():\n",
    "    video_base = l2norm(video_emb.astype(np.float32))\n",
    "\n",
    "le_audio = LabelEncoder().fit(species_audio)\n",
    "y_audio = le_audio.transform(species_audio)\n",
    "le_video = LabelEncoder().fit(species_video)\n",
    "y_video = le_video.transform(species_video)\n",
    "\n",
    "print(\"=== AMI/NMI ===\")\n",
    "\n",
    "# Audio-only\n",
    "ami_a, nmi_a = cluster_metrics(audio_base, y_audio, n_clusters=len(le_audio.classes_))\n",
    "print(f\"audio-only | AMI={ami_a:.4f}  NMI={nmi_a:.4f}\")\n",
    "\n",
    "# Video-only\n",
    "ami_v, nmi_v = cluster_metrics(video_base, y_video, n_clusters=len(le_video.classes_))\n",
    "print(f\"video-only | AMI={ami_v:.4f}  NMI={nmi_v:.4f}\")\n",
    "\n",
    "# Learned joint (audio and video separately)\n",
    "ami_ja, nmi_ja = cluster_metrics(audio_joint, y_audio, n_clusters=len(le_audio.classes_))\n",
    "ami_jv, nmi_jv = cluster_metrics(video_joint, y_video, n_clusters=len(le_video.classes_))\n",
    "print(f\"joint(audio) | AMI={ami_ja:.4f}  NMI={nmi_ja:.4f}\")\n",
    "print(f\"joint(video) | AMI={ami_jv:.4f}  NMI={nmi_jv:.4f}\")\n",
    "\n",
    "# Optional combined joint\n",
    "combined_joint = np.vstack([audio_joint, video_joint])\n",
    "combined_labels = np.concatenate([y_audio, y_video])\n",
    "ami_c, nmi_c = cluster_metrics(combined_joint, combined_labels, n_clusters=len(np.unique(combined_labels)))\n",
    "print(f\"joint(combined) | AMI={ami_c:.4f}  NMI={nmi_c:.4f}\")\n",
    "\n",
    "# Concat baseline if available\n",
    "if concat_available:\n",
    "    le_combo = LabelEncoder().fit(concat_species)\n",
    "    y_combo = le_combo.transform(concat_species)\n",
    "    combo = np.concatenate([l2norm(concat_audio.astype(np.float32)), l2norm(concat_video.astype(np.float32))], axis=1)\n",
    "    ami_c2, nmi_c2 = cluster_metrics(combo, y_combo, n_clusters=len(le_combo.classes_))\n",
    "    print(f\"concat | AMI={ami_c2:.4f}  NMI={nmi_c2:.4f}\")\n",
    "else:\n",
    "    print(\"concat | skipped (no aligned pairs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d86d194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear Probe (LogReg; 40% train) ===\n",
      "linear_probe: dropped 3 samples from classes with <2 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(89606) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89607) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89608) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89609) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89610) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89611) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89612) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89613) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89614) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(89615) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-only | acc=0.1615  macroF1=0.0896\n",
      "linear_probe: dropped 3 samples from classes with <2 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video-only | acc=0.5696  macroF1=0.4036\n",
      "linear_probe: dropped 3 samples from classes with <2 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint(audio) | acc=0.6231  macroF1=0.4654\n",
      "linear_probe: dropped 3 samples from classes with <2 examples\n",
      "joint(video) | acc=0.7595  macroF1=0.5994\n",
      "linear_probe: dropped 3 samples from classes with <2 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat | acc=0.3692  macroF1=0.2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/fish/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "# Linear classifier evaluation (40% train, stratified)\n",
    "\n",
    "print(\"\\n=== Linear Probe (LogReg; 40% train) ===\")\n",
    "\n",
    "acc, f1 = linear_probe(audio_base, y_audio, train_frac=0.4, seed=seed)\n",
    "print(f\"audio-only | acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "\n",
    "acc, f1 = linear_probe(video_base, y_video, train_frac=0.4, seed=seed)\n",
    "print(f\"video-only | acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "\n",
    "acc, f1 = linear_probe(audio_joint, y_audio, train_frac=0.4, seed=seed)\n",
    "print(f\"joint(audio) | acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "\n",
    "acc, f1 = linear_probe(video_joint, y_video, train_frac=0.4, seed=seed)\n",
    "print(f\"joint(video) | acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "\n",
    "if concat_available:\n",
    "    le_combo = LabelEncoder().fit(concat_species)\n",
    "    y_combo = le_combo.transform(concat_species)\n",
    "    combo = np.concatenate([l2norm(concat_audio.astype(np.float32)), l2norm(concat_video.astype(np.float32))], axis=1)\n",
    "    acc, f1 = linear_probe(combo, y_combo, train_frac=0.4, seed=seed)\n",
    "    print(f\"concat | acc={acc:.4f}  macroF1={f1:.4f}\")\n",
    "else:\n",
    "    print(\"concat | skipped (no aligned pairs)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
